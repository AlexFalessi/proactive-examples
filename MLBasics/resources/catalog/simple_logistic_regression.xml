<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.8"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.8 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.8/schedulerjob.xsd"
    name="simple_logistic_regression" projectName="Basic Machine Learning"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <description>
    <![CDATA[ Simple example of Logistic Regression. This demo uses the new Python Script Engine on Docker. ]]>
  </description>
  <genericInformation>
    <info name="Documentation" value="http://activeeon.com/resources/automated-machine-learning-activeeon.pdf"/>
  </genericInformation>
  <taskFlow>
    <task name="Load_Data">
      <description>
        <![CDATA[ The simplest task, ran by a python engine. ]]>
      </description>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Load_Data")

import pandas as pd

file_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"
columns_name = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = pd.read_csv(file_url, names=columns_name)

variables.put("DATAFRAME_JSON", dataframe.to_json())
variables.put("COLUMNS_NAME_JSON", pd.Series(columns_name).to_json())

print("END Load_Data")
]]>
          </code>
        </script>
      </scriptExecutable>
      <post>
        <script>
          <code language="groovy">
            <![CDATA[
variables.put("PREVIOUS_PA_TASK_NAME", variables.get("PA_TASK_NAME"))
]]>
          </code>
        </script>
      </post>
    </task>
    <task name="Train_Model">
      <description>
        <![CDATA[ The simplest task, ran by a python engine. ]]>
      </description>
      <depends>
        <task ref="LogisticRegression"/>
        <task ref="Split_Data"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Train_Model")
import pandas as pd
import random
import pickle

ALGORITHM_NAME      = variables.get("ALGORITHM_NAME")
DATA_TRAIN_DF_JSON  = variables.get("DATA_TRAIN_DF_JSON")
LABEL_TRAIN_DF_JSON = variables.get("LABEL_TRAIN_DF_JSON")

data_train_df  = pd.read_json(DATA_TRAIN_DF_JSON)
label_train_df = pd.read_json(LABEL_TRAIN_DF_JSON)

model = None
if ALGORITHM_NAME == 'SVM':
  from sklearn.svm import SVC
  model = SVC()

if ALGORITHM_NAME == 'LinearSVM':
  from sklearn.svm import LinearSVC
  model = LinearSVC(random_state=0)

if ALGORITHM_NAME == 'LogisticRegression':
  from sklearn.linear_model import LogisticRegression
  model = LogisticRegression()

model.fit(data_train_df.values, label_train_df.values.ravel())

filename = variables.get("PA_SCHEDULER_HOME")+"/"+str(random.randrange(100000, 999999))+'.model'
pickle.dump(model, open(filename, 'wb'))

variables.put("MODEL", filename)
print("END Train_Model")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Download_Model">
      <description>
        <![CDATA[ The simplest task, ran by a python engine. ]]>
      </description>
      <depends>
        <task ref="Train_Model"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Download_Model")

filename = variables.get("MODEL")

in_file = open(filename, "rb")
data = in_file.read()
in_file.close()

result = data
resultMetadata.put("file.extension", ".model")
resultMetadata.put("file.name", "myModel.model")
resultMetadata.put("content.type", "application/octet-stream")
print("END Download_Model")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="LogisticRegression">
      <description>
        <![CDATA[ The simplest task, ran by a python engine. ]]>
      </description>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
variables.put("ALGORITHM_NAME", "LogisticRegression")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Split_Data">
      <description>
        <![CDATA[ The simplest task, ran by a python engine. ]]>
      </description>
      <variables>
        <variable name="TRAIN_SIZE" value="0.7" inherited="false" />
      </variables>
      <depends>
        <task ref="Load_Data"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Split_Data")
import pandas as pd
from sklearn import model_selection

DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
dataframe = pd.read_json(DATAFRAME_JSON)
data  = dataframe.values[:,0:8]
label = dataframe.values[:,8]

COLUMNS_NAME_JSON = variables.get("COLUMNS_NAME_JSON")
columns_name_df = pd.read_json(COLUMNS_NAME_JSON,typ='series')
columns_name = columns_name_df.values

train_size = float(variables.get("TRAIN_SIZE"))
print("TRAIN_SIZE: ", train_size)
test_size = 1 - train_size
data_train, data_test, label_train, label_test = model_selection.train_test_split(data, label, test_size=test_size)

data_train_df = pd.DataFrame(data=data_train,columns=columns_name[0:8])
label_train_df = pd.DataFrame(data=label_train,columns=[columns_name[8]])

data_test_df = pd.DataFrame(data=data_test,columns=columns_name[0:8])
label_test_df = pd.DataFrame(data=label_test,columns=[columns_name[8]])

variables.put("DATA_TRAIN_DF_JSON", data_train_df.to_json())
variables.put("LABEL_TRAIN_DF_JSON", label_train_df.to_json())
variables.put("DATA_TEST_DF_JSON", data_test_df.to_json())
variables.put("LABEL_TEST_DF_JSON", label_test_df.to_json())

print("END Split_Data")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Prediction_Model">
      <description>
        <![CDATA[ The simplest task, ran by a python engine. ]]>
      </description>
      <depends>
        <task ref="Train_Model"/>
        <task ref="Split_Data"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Prediction_Model")
import pandas as pd
from sklearn.externals import joblib

DATA_TEST_DF_JSON  = variables.get("DATA_TEST_DF_JSON")
LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON")

data_test_df  = pd.read_json(DATA_TEST_DF_JSON)
label_test_df = pd.read_json(LABEL_TEST_DF_JSON)

filename = variables.get("MODEL")

loaded_model = joblib.load(filename)
score = loaded_model.score(data_test_df.values, label_test_df.values.ravel())
print("MODEL SCORE: ", score)

print("END Prediction_Model")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
  </taskFlow>
</job>